# Sentimental-Analysis-using-BERT-with-Deep-Learning
BERT: Bidirectional Encoder Representation from Transformer.

BERTis a neural network-based technique for natural language processing pre-training.
In-short, it can be used to help Google better discern the context of words in search queries.


Three Main Objectives of this Project:

1.Sentimental Analysis, how to approach problem from a Neural Network.

2.Loading in Pre-trained BERT with custom output Layer.

3.Train and Evaluate finetuned BERT architecture on sentimental analysis.

Tasks:
1) Exploratory Data Analysis and preprocessing.
2) Training/Validation
3) Loading Tokenizer and Encoding our Data.
4) Setting BERT pre-trained Model.
5) Creating Data Loaders.
6) Setting up Optimizer and Scheduler.
7) Definin our performances of metrices
8) Creat Training Loop.
9) Loading and Evaluating Model.
